{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2085f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_data\n",
    "\n",
    "df_raw_AAPL = load_data('AAPL', '2009-01-01', '2025-01-01')\n",
    "df_train_AAPL = df_raw_AAPL[df_raw_AAPL.index <= '2023-01-01']\n",
    "df_test_AAPL = df_raw_AAPL[df_raw_AAPL.index > '2023-01-01']\n",
    "\n",
    "df_raw_AAPL.to_excel('aapl.xlsx')\n",
    "df_train_AAPL.to_parquet('data/aapl_train.parquet')\n",
    "df_test_AAPL.to_parquet('data/aapl_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251e35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import numpy as np\n",
    "\n",
    "def reward_function(history):\n",
    "    return np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2]) #log (p_t / p_t-1 )\n",
    "\n",
    "env_train = gym.make(\n",
    "        \"TradingEnv\",\n",
    "        name= \"AAPL\",\n",
    "        dataset_path = 'C:/Users/jasiu/Desktop/dqn_trading_masters/data/aapl_train.parquet',\n",
    "        windows= 21,\n",
    "        positions = [-1, 0, 1], # From -1 (=SHORT), to +1 (=LONG)\n",
    "        initial_position = 'random', #Initial position\n",
    "        trading_fees = 0.04/100, # 0.2% per stock buy / sell\n",
    "        borrow_interest_rate = 0.11 / 252,\n",
    "        reward_function = reward_function,\n",
    "        portfolio_initial_value = 10_000, # in FIAT (here, USD)\n",
    "        max_episode_duration = 126,\n",
    "        disable_env_checker= True,\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d141057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 19:11:09.951135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-08 19:11:10.714914: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "[I 2026-01-08 19:11:14,350] A new study created in memory with name: no-name-e511f440-79bf-4795-b00e-482ec7c96a53\n",
      "[I 2026-01-08 19:17:15,660] Trial 0 finished with value: 0.13969 and parameters: {'one_minus_gamma': 0.010690757502486941, 'batch_size_pow': 9, 'learning_rate': 0.00016607700346357455, 'train_freq': 2, 'subsample_steps': 2, 'exploration_final_eps': 0.12848698598436992, 'exploration_fraction': 0.0325560925310141, 'target_update_interval': 69, 'net_arch': 'small'}. Best is trial 0 with value: 0.13969.\n",
      "[I 2026-01-08 19:21:37,141] Trial 2 finished with value: 0.051083 and parameters: {'one_minus_gamma': 0.00029812158782004686, 'batch_size_pow': 9, 'learning_rate': 0.000738928095150621, 'train_freq': 9, 'subsample_steps': 4, 'exploration_final_eps': 0.0753436216931154, 'exploration_fraction': 0.18256200192119548, 'target_update_interval': 678, 'net_arch': 'medium'}. Best is trial 0 with value: 0.13969.\n",
      "[I 2026-01-08 19:21:58,647] Trial 1 finished with value: 0.235801 and parameters: {'one_minus_gamma': 0.018328376445735414, 'batch_size_pow': 3, 'learning_rate': 0.00019576514365757244, 'train_freq': 1, 'subsample_steps': 1, 'exploration_final_eps': 0.1211011640731392, 'exploration_fraction': 0.4817279485612022, 'target_update_interval': 1, 'net_arch': 'big'}. Best is trial 1 with value: 0.235801.\n",
      "[I 2026-01-08 19:25:30,315] Trial 3 finished with value: -0.53675 and parameters: {'one_minus_gamma': 0.0014418729389536944, 'batch_size_pow': 6, 'learning_rate': 0.00019742912874911017, 'train_freq': 3, 'subsample_steps': 2, 'exploration_final_eps': 0.10933830685378641, 'exploration_fraction': 0.21710074793322492, 'target_update_interval': 19788, 'net_arch': 'big'}. Best is trial 1 with value: 0.235801.\n",
      "[I 2026-01-08 19:27:35,784] Trial 5 finished with value: 0.0 and parameters: {'one_minus_gamma': 0.0002698416527993065, 'batch_size_pow': 7, 'learning_rate': 0.00043877754312401923, 'train_freq': 10, 'subsample_steps': 8, 'exploration_final_eps': 0.18415953891676312, 'exploration_fraction': 0.4787048179528262, 'target_update_interval': 9, 'net_arch': 'big'}. Best is trial 1 with value: 0.235801.\n",
      "[I 2026-01-08 19:28:24,601] Trial 4 finished with value: 0.571459 and parameters: {'one_minus_gamma': 0.016708540218086633, 'batch_size_pow': 2, 'learning_rate': 0.001956345320525982, 'train_freq': 7, 'subsample_steps': 1, 'exploration_final_eps': 0.08166095738108024, 'exploration_fraction': 0.33267672505794693, 'target_update_interval': 2948, 'net_arch': 'small'}. Best is trial 4 with value: 0.571459.\n",
      "[I 2026-01-08 19:30:49,459] Trial 6 finished with value: -0.139119 and parameters: {'one_minus_gamma': 0.026608978465355926, 'batch_size_pow': 11, 'learning_rate': 0.0002977848767783782, 'train_freq': 9, 'subsample_steps': 7, 'exploration_final_eps': 0.07339350047849556, 'exploration_fraction': 0.3665631366670439, 'target_update_interval': 15, 'net_arch': 'small'}. Best is trial 4 with value: 0.571459.\n",
      "[I 2026-01-08 19:31:42,769] Trial 7 finished with value: 0.616145 and parameters: {'one_minus_gamma': 0.0006251619063727837, 'batch_size_pow': 2, 'learning_rate': 7.134329517076923e-05, 'train_freq': 3, 'subsample_steps': 2, 'exploration_final_eps': 0.013981162673719983, 'exploration_fraction': 0.1457021285825098, 'target_update_interval': 234, 'net_arch': 'medium'}. Best is trial 7 with value: 0.616145.\n",
      "[I 2026-01-08 19:35:04,673] Trial 9 finished with value: -0.26664 and parameters: {'one_minus_gamma': 0.01235293505770377, 'batch_size_pow': 8, 'learning_rate': 2.5437773635777596e-05, 'train_freq': 5, 'subsample_steps': 3, 'exploration_final_eps': 0.038537169966860275, 'exploration_fraction': 0.13654298849880708, 'target_update_interval': 5980, 'net_arch': 'big'}. Best is trial 7 with value: 0.616145.\n",
      "[I 2026-01-08 19:44:22,254] Trial 10 finished with value: -0.62518 and parameters: {'one_minus_gamma': 0.0022473229351794586, 'batch_size_pow': 10, 'learning_rate': 4.0016478091127686e-05, 'train_freq': 7, 'subsample_steps': 3, 'exploration_final_eps': 0.09500683152478893, 'exploration_fraction': 0.4276890952364851, 'target_update_interval': 3, 'net_arch': 'big'}. Best is trial 7 with value: 0.616145.\n",
      "[I 2026-01-08 19:44:57,532] Trial 8 finished with value: 0.055683 and parameters: {'one_minus_gamma': 0.00018982980950236773, 'batch_size_pow': 8, 'learning_rate': 0.0005192479079543657, 'train_freq': 2, 'subsample_steps': 1, 'exploration_final_eps': 0.09504592849672192, 'exploration_fraction': 0.3537890701484294, 'target_update_interval': 537, 'net_arch': 'big'}. Best is trial 7 with value: 0.616145.\n",
      "[I 2026-01-08 19:49:14,667] Trial 11 finished with value: 0.060158 and parameters: {'one_minus_gamma': 0.00031216787177569256, 'batch_size_pow': 10, 'learning_rate': 0.00015051451858056583, 'train_freq': 5, 'subsample_steps': 5, 'exploration_final_eps': 0.15998758090080728, 'exploration_fraction': 0.3409245196911916, 'target_update_interval': 3029, 'net_arch': 'big'}. Best is trial 7 with value: 0.616145.\n",
      "Exception ignored in: <_io.FileIO name=6 mode='wb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jasiu\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\utils\\_process_win32.py\", line 124, in system\n",
      "    return process_handler(cmd, _system_body)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedWriter name=6>\n",
      "Exception ignored in: <_io.FileIO name=7 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jasiu\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\utils\\_process_win32.py\", line 124, in system\n",
      "    return process_handler(cmd, _system_body)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=7>\n",
      "Exception ignored in: <_io.FileIO name=8 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jasiu\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\utils\\_process_win32.py\", line 124, in system\n",
      "    return process_handler(cmd, _system_body)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TradingEnv ==========\n",
      "Seed: 42\n",
      "Loading hyperparameters from: c:\\Users\\jasiu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rl_zoo3\\hyperparams\\dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 128),\n",
      "             ('buffer_size', 100000),\n",
      "             ('exploration_final_eps', 0.02),\n",
      "             ('exploration_fraction', 0.1),\n",
      "             ('gamma', 0.99),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.0003),\n",
      "             ('learning_starts', 2000),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256])'),\n",
      "             ('target_update_interval', 1000),\n",
      "             ('tau', 1.0),\n",
      "             ('train_freq', 1)])\n",
      "Using 1 environments\n",
      "Overwriting n_timesteps with n=100000\n",
      "Doing 1 intermediate evaluations for pruning based on the number of timesteps. (1 evaluation every 100k timesteps)\n",
      "Optimizing hyperparameters\n",
      "Sampler: random - Pruner: median\n",
      "Number of finished trials:  12\n",
      "Best trial:\n",
      "Value:  0.616145\n",
      "Params: \n",
      "    one_minus_gamma: 0.0006251619063727837\n",
      "    batch_size_pow: 2\n",
      "    learning_rate: 7.134329517076923e-05\n",
      "    train_freq: 3\n",
      "    subsample_steps: 2\n",
      "    exploration_final_eps: 0.013981162673719983\n",
      "    exploration_fraction: 0.1457021285825098\n",
      "    target_update_interval: 234\n",
      "    net_arch: medium\n",
      "User Attributes: \n",
      "    gamma: 0.9993748380936273\n",
      "    batch_size: 4\n",
      "Writing report to logs/aapl_dqn_opt\\dqn\\report_TradingEnv_12-trials-100000-random-median_1767898154\n"
     ]
    }
   ],
   "source": [
    "!python -m rl_zoo3.train --algo dqn --env TradingEnv --gym-packages gym_trading_env \\\n",
    "  -optimize --n-trials 12 --n-jobs 2 --sampler random --pruner median \\\n",
    "  -n 100000 \\\n",
    "  --eval-freq 10000 \\\n",
    "  --eval-episodes 5 \\\n",
    "  --seed 42 \\\n",
    "  --uuid \\\n",
    "  --device auto \\\n",
    "  --vec-env dummy \\\n",
    "  -f logs/aapl_dqn_opt \\\n",
    "  --env-kwargs \\\n",
    "  dataset_path:\"'C:/Users/jasiu/Desktop/dqn_trading_masters/data/aapl_train.parquet'\" \\\n",
    "  windows:21 \\\n",
    "  positions:\"[-1, 0, 1]\" \\\n",
    "  initial_position:0 \\\n",
    "  trading_fees:0.0004 \\\n",
    "  borrow_interest_rate:0.0004365079365079365 \\\n",
    "  portfolio_initial_value:10000 \\\n",
    "  max_episode_duration:\"'max'\" \\\n",
    "  disable_env_checker:True \\\n",
    "  verbose:0 \\\n",
    "  --eval-env-kwargs \\\n",
    "  dataset_path:\"'C:/Users/jasiu/Desktop/dqn_trading_masters/data/aapl_test.parquet'\" \\\n",
    "  windows:21 \\\n",
    "  positions:\"[-1, 0, 1]\" \\\n",
    "  initial_position:0 \\\n",
    "  trading_fees:0.0004 \\\n",
    "  borrow_interest_rate:0.0004365079365079365 \\\n",
    "  portfolio_initial_value:10000 \\\n",
    "  max_episode_duration:\"'max'\" \\\n",
    "  disable_env_checker:True \\\n",
    "  verbose:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c721796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TradingEnv ==========\n",
      "Seed: 42\n",
      "Loading hyperparameters from: c:\\Users\\jasiu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rl_zoo3\\hyperparams\\dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 4),\n",
      "             ('buffer_size', 100000),\n",
      "             ('exploration_final_eps', 0.013981162673719983),\n",
      "             ('exploration_fraction', 0.1457021285825098),\n",
      "             ('gamma', 0.9993748380936273),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 7.134329517076923e-05),\n",
      "             ('learning_starts', 2000),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[128, 64])'),\n",
      "             ('target_update_interval', 234),\n",
      "             ('tau', 1.0),\n",
      "             ('train_freq', 3)])\n",
      "Using 1 environments\n",
      "Creating test environment\n",
      "Using cpu device\n",
      "Log path: logs/aapl_dqn/dqn/TradingEnv_1_7e3ff8d8-f144-4317-b31b-d4851bac592d\n",
      "Logging to runs/aapl_dqn\\TradingEnv\\DQN_11\n",
      "Eval num_timesteps=10000, episode_reward=0.15 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.146    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.932    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00194  |\n",
      "|    n_updates        | 2667     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.26    |\n",
      "|    exploration_rate | 0.907    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1311     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 13760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 3920     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=0.22 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.217    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.865    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00114  |\n",
      "|    n_updates        | 6000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1231     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 27520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00911  |\n",
      "|    n_updates        | 8507     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.193   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00776  |\n",
      "|    n_updates        | 9333     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.287   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.729    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0603   |\n",
      "|    n_updates        | 12667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.32    |\n",
      "|    exploration_rate | 0.721    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1148     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 41280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 13093    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.65 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.652   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.662    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 16000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1138     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 55040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0165   |\n",
      "|    n_updates        | 17680    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.158   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00958  |\n",
      "|    n_updates        | 19333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.45    |\n",
      "|    exploration_rate | 0.534    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1132     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 68800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.00407  |\n",
      "|    n_updates        | 22267    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=0.41 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.408    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.526    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 22667    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0394  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0744   |\n",
      "|    n_updates        | 26000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.37    |\n",
      "|    exploration_rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1100     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 82560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 26853    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.177   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.391    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0808   |\n",
      "|    n_updates        | 29333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    exploration_rate | 0.348    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1095     |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 96320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.065    |\n",
      "|    n_updates        | 31440    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.43 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.427   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.323    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0422   |\n",
      "|    n_updates        | 32667    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-0.26 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.258   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.256    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0379   |\n",
      "|    n_updates        | 36000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    exploration_rate | 0.255    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1074     |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 110080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 36027    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-0.45 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.445   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 39333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.19    |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1067     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 123840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0416   |\n",
      "|    n_updates        | 40613    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.108   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.12     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0256   |\n",
      "|    n_updates        | 42667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    exploration_rate | 0.0688   |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1063     |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 137600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.175    |\n",
      "|    n_updates        | 45200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.00674  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0526   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0224   |\n",
      "|    n_updates        | 46000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=0.16 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.163    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.467    |\n",
      "|    n_updates        | 49333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.952   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1047     |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 151360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 49787    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.45 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.448    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.123    |\n",
      "|    n_updates        | 52667    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.864   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1042     |\n",
      "|    time_elapsed     | 158      |\n",
      "|    total_timesteps  | 165120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0923   |\n",
      "|    n_updates        | 54373    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-0.40 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.402   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 56000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.799   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1038     |\n",
      "|    time_elapsed     | 172      |\n",
      "|    total_timesteps  | 178880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0674   |\n",
      "|    n_updates        | 58960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.039   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0412   |\n",
      "|    n_updates        | 59333    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-0.49 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.486   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 62667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.722   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1028     |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total_timesteps  | 192640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.284    |\n",
      "|    n_updates        | 63547    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.283   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 66000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.721   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1024     |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 206400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.391    |\n",
      "|    n_updates        | 68133    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0228  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 69333    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=0.24 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.238    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 72667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.698   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1016     |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 220160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.31     |\n",
      "|    n_updates        | 72720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=0.19 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.189    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 76000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.676   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1014     |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 233920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 77307    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0387  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 79333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.659   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1011     |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 247680   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 81893    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0186   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.536    |\n",
      "|    n_updates        | 82667    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-0.85 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.851   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.6      |\n",
      "|    n_updates        | 86000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.656   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1001     |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 261440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 86480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=0.21 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.208    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.526    |\n",
      "|    n_updates        | 89333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.615   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 998      |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 275200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 91067    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-0.69 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.692   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0907   |\n",
      "|    n_updates        | 92667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.678   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 996      |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 288960   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.342    |\n",
      "|    n_updates        | 95653    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0338  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.571    |\n",
      "|    n_updates        | 96000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0315  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.214    |\n",
      "|    n_updates        | 99333    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.634   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 989      |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 302720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0983   |\n",
      "|    n_updates        | 100240   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-0.58 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.58    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.553    |\n",
      "|    n_updates        | 102667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.613   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 987      |\n",
      "|    time_elapsed     | 320      |\n",
      "|    total_timesteps  | 316480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.205    |\n",
      "|    n_updates        | 104827   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=0.10 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.101    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.542    |\n",
      "|    n_updates        | 106000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0101  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 109333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.578   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 981      |\n",
      "|    time_elapsed     | 336      |\n",
      "|    total_timesteps  | 330240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.71     |\n",
      "|    n_updates        | 109413   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0882   |\n",
      "|    n_updates        | 112667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.568   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 980      |\n",
      "|    time_elapsed     | 350      |\n",
      "|    total_timesteps  | 344000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 114000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.406   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 116000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.556   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 979      |\n",
      "|    time_elapsed     | 365      |\n",
      "|    total_timesteps  | 357760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.419    |\n",
      "|    n_updates        | 118587   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0638   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.393    |\n",
      "|    n_updates        | 119333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0775   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.51     |\n",
      "|    n_updates        | 122667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.483   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 973      |\n",
      "|    time_elapsed     | 381      |\n",
      "|    total_timesteps  | 371520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 123173   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.133   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.739    |\n",
      "|    n_updates        | 126000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.448   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 972      |\n",
      "|    time_elapsed     | 396      |\n",
      "|    total_timesteps  | 385280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.583    |\n",
      "|    n_updates        | 127760   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.00451 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.248    |\n",
      "|    n_updates        | 129333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.344   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 972      |\n",
      "|    time_elapsed     | 410      |\n",
      "|    total_timesteps  | 399040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.675    |\n",
      "|    n_updates        | 132347   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.229   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.47     |\n",
      "|    n_updates        | 132667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0836   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 136000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.331   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 968      |\n",
      "|    time_elapsed     | 426      |\n",
      "|    total_timesteps  | 412800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 136933   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-0.36 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.356   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 139333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.342   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 967      |\n",
      "|    time_elapsed     | 440      |\n",
      "|    total_timesteps  | 426560   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 141520   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=0.34 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.344    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.865    |\n",
      "|    n_updates        | 142667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=0.13 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.135    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 440000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 146000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.283   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 964      |\n",
      "|    time_elapsed     | 456      |\n",
      "|    total_timesteps  | 440320   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 146107   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.871   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0833   |\n",
      "|    n_updates        | 149333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.273   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 963      |\n",
      "|    time_elapsed     | 471      |\n",
      "|    total_timesteps  | 454080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 150693   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=0.09 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.095    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 460000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.889    |\n",
      "|    n_updates        | 152667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.279   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 963      |\n",
      "|    time_elapsed     | 485      |\n",
      "|    total_timesteps  | 467840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 155280   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.193   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 470000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 156000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-1.25 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -1.25    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 480000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.908    |\n",
      "|    n_updates        | 159333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.352   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 959      |\n",
      "|    time_elapsed     | 501      |\n",
      "|    total_timesteps  | 481600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.895    |\n",
      "|    n_updates        | 159867   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-0.80 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.802   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 490000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.296    |\n",
      "|    n_updates        | 162667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.391   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 960      |\n",
      "|    time_elapsed     | 515      |\n",
      "|    total_timesteps  | 495360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.404    |\n",
      "|    n_updates        | 164453   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.00711  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 166000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.376   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 959      |\n",
      "|    time_elapsed     | 530      |\n",
      "|    total_timesteps  | 509120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.543    |\n",
      "|    n_updates        | 169040   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-0.57 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.568   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 510000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.621    |\n",
      "|    n_updates        | 169333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=0.35 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.348    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 520000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 172667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.389   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 956      |\n",
      "|    time_elapsed     | 546      |\n",
      "|    total_timesteps  | 522880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 173627   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-0.66 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.657   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 530000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 176000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.419   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 956      |\n",
      "|    time_elapsed     | 560      |\n",
      "|    total_timesteps  | 536640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 178213   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0782  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 540000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 179333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.166   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 182667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.427   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 954      |\n",
      "|    time_elapsed     | 576      |\n",
      "|    total_timesteps  | 550400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.865    |\n",
      "|    n_updates        | 182800   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-0.52 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.523   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 560000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 186000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.428   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 953      |\n",
      "|    time_elapsed     | 591      |\n",
      "|    total_timesteps  | 564160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 187387   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 570000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 189333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.489   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 954      |\n",
      "|    time_elapsed     | 605      |\n",
      "|    total_timesteps  | 577920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.461    |\n",
      "|    n_updates        | 191973   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.372   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 580000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.0839   |\n",
      "|    n_updates        | 192667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=0.07 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0672   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 590000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 196000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.467   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 951      |\n",
      "|    time_elapsed     | 621      |\n",
      "|    total_timesteps  | 591680   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.85     |\n",
      "|    n_updates        | 196560   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.409   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 199333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.493   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 951      |\n",
      "|    time_elapsed     | 636      |\n",
      "|    total_timesteps  | 605440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 201147   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-0.86 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.858   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 610000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 202667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.51    |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 951      |\n",
      "|    time_elapsed     | 650      |\n",
      "|    total_timesteps  | 619200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 205733   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0246  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 620000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 206000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.05    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 630000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.93     |\n",
      "|    n_updates        | 209333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.484   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 666      |\n",
      "|    total_timesteps  | 632960   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 210320   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-0.54 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.537   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 640000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.8      |\n",
      "|    n_updates        | 212667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.512   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 681      |\n",
      "|    total_timesteps  | 646720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 214907   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.194   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 216000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0682  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 660000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.853    |\n",
      "|    n_updates        | 219333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.523   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 947      |\n",
      "|    time_elapsed     | 697      |\n",
      "|    total_timesteps  | 660480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 219493   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.124   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 670000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.751    |\n",
      "|    n_updates        | 222667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.57    |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 947      |\n",
      "|    time_elapsed     | 711      |\n",
      "|    total_timesteps  | 674240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 224080   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=0.46 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.462    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 680000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 226000   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.577   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 725      |\n",
      "|    total_timesteps  | 688000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.564    |\n",
      "|    n_updates        | 228667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0338  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 690000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.613    |\n",
      "|    n_updates        | 229333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.411   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 232667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.594   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 947      |\n",
      "|    time_elapsed     | 740      |\n",
      "|    total_timesteps  | 701760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 233253   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-0.40 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.398   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 710000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.65     |\n",
      "|    n_updates        | 236000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.582   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 754      |\n",
      "|    total_timesteps  | 715520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 237840   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-1.35 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -1.35    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 720000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 239333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.621   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 768      |\n",
      "|    total_timesteps  | 729280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 242427   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 730000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.6      |\n",
      "|    n_updates        | 242667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-0.26 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.256   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 740000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.979    |\n",
      "|    n_updates        | 246000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.637   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 783      |\n",
      "|    total_timesteps  | 743040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 247013   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-0.73 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.729   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.04     |\n",
      "|    n_updates        | 249333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.654   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 797      |\n",
      "|    total_timesteps  | 756800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 251600   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=0.30 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.3      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 760000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 252667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0465   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 770000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.738    |\n",
      "|    n_updates        | 256000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.608   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 812      |\n",
      "|    total_timesteps  | 770560   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 256187   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-0.95 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.948   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 780000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.85     |\n",
      "|    n_updates        | 259333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.621   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 826      |\n",
      "|    total_timesteps  | 784320   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 260773   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.081   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 790000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.514    |\n",
      "|    n_updates        | 262667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.648   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 840      |\n",
      "|    total_timesteps  | 798080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 265360   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-0.84 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.842   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.37     |\n",
      "|    n_updates        | 266000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=0.52 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.518    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 810000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 269333   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.679   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 855      |\n",
      "|    total_timesteps  | 811840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.99     |\n",
      "|    n_updates        | 269947   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.00915  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 820000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 272667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.583   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 869      |\n",
      "|    total_timesteps  | 825600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.664    |\n",
      "|    n_updates        | 274533   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.00772 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 830000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.45     |\n",
      "|    n_updates        | 276000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.564   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 950      |\n",
      "|    time_elapsed     | 883      |\n",
      "|    total_timesteps  | 839360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 279120   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.0954  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 840000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.23     |\n",
      "|    n_updates        | 279333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-0.56 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.559   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.72     |\n",
      "|    n_updates        | 282667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.64    |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 898      |\n",
      "|    total_timesteps  | 853120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 283707   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-1.35 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -1.35    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 860000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.16     |\n",
      "|    n_updates        | 286000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.678   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 950      |\n",
      "|    time_elapsed     | 912      |\n",
      "|    total_timesteps  | 866880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.94     |\n",
      "|    n_updates        | 288293   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.866   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 870000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 289333   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.867   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 880000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 292667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.688   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 927      |\n",
      "|    total_timesteps  | 880640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 4.16     |\n",
      "|    n_updates        | 292880   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.052   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 890000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 296000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.709   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 950      |\n",
      "|    time_elapsed     | 941      |\n",
      "|    total_timesteps  | 894400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 3.05     |\n",
      "|    n_updates        | 297467   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=0.55 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.554    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.08     |\n",
      "|    n_updates        | 299333   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.67    |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 950      |\n",
      "|    time_elapsed     | 955      |\n",
      "|    total_timesteps  | 908160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 302053   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=0.45 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.452    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 910000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 3.44     |\n",
      "|    n_updates        | 302667   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.078   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 920000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 306000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.636   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 971      |\n",
      "|    total_timesteps  | 921920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.722    |\n",
      "|    n_updates        | 306640   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=0.21 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.212    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 930000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 3.06     |\n",
      "|    n_updates        | 309333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.677   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 985      |\n",
      "|    total_timesteps  | 935680   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 3.16     |\n",
      "|    n_updates        | 311227   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-0.77 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.769   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 940000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 312667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.627   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 1000     |\n",
      "|    total_timesteps  | 949440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.73     |\n",
      "|    n_updates        | 315813   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-0.34 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.342   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.664    |\n",
      "|    n_updates        | 316000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-0.75 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.745   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 960000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 3.74     |\n",
      "|    n_updates        | 319333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.628   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 1015     |\n",
      "|    total_timesteps  | 963200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.707    |\n",
      "|    n_updates        | 320400   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0196   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 970000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 322667   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.573   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 948      |\n",
      "|    time_elapsed     | 1030     |\n",
      "|    total_timesteps  | 976960   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 324987   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-0.51 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.514   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 980000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 326000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | -0.135   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 990000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 2.16     |\n",
      "|    n_updates        | 329333   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44e+03 |\n",
      "|    ep_rew_mean      | -0.603   |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 947      |\n",
      "|    time_elapsed     | 1046     |\n",
      "|    total_timesteps  | 990720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 329573   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 481.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 481      |\n",
      "|    mean_reward      | 0.0132   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.014    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 7.13e-05 |\n",
      "|    loss             | 0.529    |\n",
      "|    n_updates        | 332667   |\n",
      "----------------------------------\n",
      "Saving to logs/aapl_dqn/dqn/TradingEnv_1_7e3ff8d8-f144-4317-b31b-d4851bac592d\n",
      "Saving replay buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 22:45:00.546239: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-08 22:45:01.281327: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Exception ignored in: <_io.FileIO name=9 mode='wb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jasiu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1014, in run\n",
      "    del self._target, self._args, self._kwargs\n",
      "        ^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedWriter name=9>\n",
      "Exception ignored in: <_io.FileIO name=10 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jasiu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1014, in run\n",
      "    del self._target, self._args, self._kwargs\n",
      "        ^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name=11 mode='rb' closefd=True>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jasiu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1014, in run\n",
      "    del self._target, self._args, self._kwargs\n",
      "        ^^^^^^^^^^^^\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name=11>\n"
     ]
    }
   ],
   "source": [
    "!python -m rl_zoo3.train --algo dqn --env TradingEnv --gym-packages gym_trading_env \\\n",
    "  -tb runs/aapl_dqn \\\n",
    "  -f logs/aapl_dqn \\\n",
    "  --eval-freq 10000 \\\n",
    "  --eval-episodes 10 \\\n",
    "  --save-freq 50000 \\\n",
    "  --save-replay-buffer \\\n",
    "  --seed 42 \\\n",
    "  --uuid \\\n",
    "  --device auto \\\n",
    "  --vec-env dummy \\\n",
    "  --env-kwargs \\\n",
    "  dataset_path:\"'C:/Users/jasiu/Desktop/dqn_trading_masters/data/aapl_train.parquet'\" \\\n",
    "  windows:21 \\\n",
    "  positions:\"[-1, 0, 1]\" \\\n",
    "  initial_position:0 \\\n",
    "  trading_fees:0.0004 \\\n",
    "  borrow_interest_rate:0.0004365079365079365 \\\n",
    "  portfolio_initial_value:10000 \\\n",
    "  max_episode_duration:\"'max'\" \\\n",
    "  disable_env_checker:True \\\n",
    "  verbose:0 \\\n",
    "  --eval-env-kwargs \\\n",
    "  dataset_path:\"'C:/Users/jasiu/Desktop/dqn_trading_masters/data/aapl_test.parquet'\" \\\n",
    "  windows:21 \\\n",
    "  positions:\"[-1, 0, 1]\" \\\n",
    "  initial_position:0 \\\n",
    "  trading_fees:0.0004 \\\n",
    "  borrow_interest_rate:0.0004365079365079365 \\\n",
    "  portfolio_initial_value:10000 \\\n",
    "  max_episode_duration:\"'max'\" \\\n",
    "  disable_env_checker:True \\\n",
    "  verbose:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b881acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs/aapl_dqn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
